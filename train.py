import argparse
import pathlib
import importlib
from conf import default, general, paths
from models.resunet import ResUnet
import os
import time
from multiprocessing import Process
import sys
from utils.dataloader import TreeTrainDataSet
from torch.utils.data import DataLoader
import torch
from torch import nn
from utils.ops import count_parameters
from tqdm import tqdm
from utils.trainer import train_loop, val_loop, EarlyStop, val_sample_image
from torch.optim.lr_scheduler import MultiStepLR

parser = argparse.ArgumentParser(
    description='Train NUMBER_MODELS models based in the same parameters'
)

parser.add_argument( # Experiment number
    '-e', '--experiment',
    type = int,
    default = 2,
    help = 'The number of the experiment'
)

parser.add_argument( # batch size
    '-b', '--batch-size',
    type = int,
    default = default.BATCH_SIZE,
    help = 'The number of samples of each batch'
)

parser.add_argument( # Number of models to be trained
    '-n', '--number-models',
    type = int,
    default = 1,
    help = 'The number models to be trained from the scratch'
)

parser.add_argument( # Experiment path
    '-c', '--continue-train',
    type = bool,
    default = False,
    help = 'Continue previous training'
)

parser.add_argument( # Experiment path
    '-x', '--experiments-path',
    type = pathlib.Path,
    default = paths.PATH_EXPERIMENTS,
    help = 'The patch to data generated by all experiments'
)

parser.add_argument( # Data Augmentation
    '-a', '--data-aug',
    type = bool,
    default = True,
    help = 'Use or not data augmentation'
)

args = parser.parse_args()



exp_path = os.path.join(str(args.experiments_path), f'exp_{args.experiment}')
if not os.path.exists(exp_path):
    os.mkdir(exp_path)

logs_path = os.path.join(exp_path, f'logs')
if not os.path.exists(logs_path):
    os.mkdir(logs_path)

models_path = os.path.join(exp_path, f'models')
if not os.path.exists(models_path):
    os.mkdir(models_path)

visual_path = os.path.join(exp_path, f'visual')
if not os.path.exists(visual_path):
    os.mkdir(visual_path)

predicted_path = os.path.join(exp_path, f'predicted')
if not os.path.exists(predicted_path):
    os.mkdir(predicted_path)

results_path = os.path.join(exp_path, f'results')
if not os.path.exists(results_path):
    os.mkdir(results_path)

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using {device} device")

outfile = os.path.join(logs_path, f'train_{args.experiment}.txt')
with open(outfile, 'w') as sys.stdout:

    path_to_patches_train = os.path.join(paths.PREPARED_PATH, 'train_patches.npy')
    path_to_patches_val = os.path.join(paths.PREPARED_PATH, 'val_patches.npy')

    ds_train = TreeTrainDataSet(path_to_patches = path_to_patches_train, device = device, data_aug=args.data_aug)
    ds_val = TreeTrainDataSet(path_to_patches = path_to_patches_val, device = device)

    dataloader_train = DataLoader(ds_train, batch_size=args.batch_size, shuffle=True)
    dataloader_val = DataLoader(ds_val, batch_size=args.batch_size, shuffle=False)

    model_m =importlib.import_module(f'conf.model_{args.experiment}')
    model = model_m.get_model()

    print(f'{model.__class__.__name__}')

    model.to(device)

    print(f'Model trainable parameters: {count_parameters(model):,}')

    torch.set_num_threads(8)

    #loss_fn = nn.CrossEntropyLoss(ignore_index=general.DISCARDED_CLASS, weight=torch.tensor(general.CLASSES_WEIGHTS).to(device))
    loss_fn = nn.CrossEntropyLoss(ignore_index=general.DISCARDED_CLASS)
    #loss_fn = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=general.LEARNING_RATE, betas = general.LEARNING_RATE_BETAS)
    #optimizer = torch.optim.SGD(model.parameters(), lr=general.LEARNING_RATE)
    model_path = os.path.join(models_path, 'model.pt')
    early_stop = EarlyStop(
        train_patience=general.EARLY_STOP_PATIENCE,
        path_to_save = model_path,
        min_delta = general.EARLY_STOP_MIN_DELTA,
        min_epochs = general.EARLY_STOP_MIN_EPOCHS
        )

    if args.continue_train:
        model.load_state_dict(torch.load(model_path))

    t0 = time.perf_counter()
    
    print(f'Model: {model}')
    print(f'Train batch size: {args.batch_size}')
    print(f'Data augmentation: {args.data_aug}')
    print(f'Loss fn: {loss_fn}')
    print(f'Optmizer :{optimizer}')
    print(f'Paticence :{general.EARLY_STOP_PATIENCE}')
    print(f'Early Stop Min Delta :{general.EARLY_STOP_MIN_DELTA}')
    print(f'Early Stop Min Epochs :{general.EARLY_STOP_MIN_EPOCHS}')
    print(f'Scheduler Gamma :{general.LEARNING_RATE_SCHEDULER_GAMMA}')
    print(f'LR Milestones :{general.LEARNING_RATE_SCHEDULER_MILESTONES}')

    scheduler = MultiStepLR(
        optimizer, 
        milestones = general.LEARNING_RATE_SCHEDULER_MILESTONES,
        gamma=general.LEARNING_RATE_SCHEDULER_GAMMA,
        verbose = True
        )
    for t in range(general.MAX_EPOCHS):
        print(f"-------------------------------\nEpoch {t+1}")
        model.train()
        train_loop(dataloader_train, model, loss_fn, optimizer)
        model.eval()
        val_loss = val_loop(dataloader_val, model, loss_fn)
        val_sample_image(dataloader_val, model, visual_path, t)
        if early_stop.testEpoch(model = model, val_value = val_loss):
            break
        scheduler.step()
    print(f'Training time: {(time.perf_counter() - t0)/60} mins')
